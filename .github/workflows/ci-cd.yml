name: CI/CD for Airflow ETL Project

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  run-tests:
    name: Continuous Integration
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Code
        uses: actions/checkout@v3

      - name: Set up Python Environment
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run Tests
        env:
          AWS_ACCESS_KEY_ID: fake_access_key
          AWS_SECRET_ACCESS_KEY: fake_secret_key
        run: pytest

  
  deploy-to-ec2:
    name: Continuous Deployment
    runs-on: ubuntu-latest
    needs: run-tests
    steps:
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4.1.0
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Deploy to EC2 via SSH
        uses: appleboy/ssh-action@v1.2.0
        with:
          host: ${{ secrets.EC2_HOST }}
          username: ${{ secrets.EC2_USERNAME }}
          key: ${{ secrets.EC2_SSH_KEY }}
          script: |
            cd /home/ubuntu
            if [ ! -d "airflow-project" ]; then
              echo "Cloning repository for the first time"
              git clone https://github.com/S-Delowar/DataFueler-automated-ETL-pipenine.git airflow-project
              cd airflow-project
              sudo chown -R 1000:0 ./dags ./logs ./plugins
            else
              echo "Repository already exists, pulling latest changes"
              cd airflow-project && git pull origin main
            fi

            echo "Stopping existing containers (if running)"
            docker-compose down

            echo "Writing environment variables to .env file"
            cat > .env <<EOF
            RDS_HOST=${{ secrets.RDS_HOST }}
            RDS_USER=${{ secrets.RDS_USER }}
            RDS_PASSWORD=${{ secrets.RDS_PASSWORD }}
            RDS_DB=${{ secrets.RDS_DB }}
            RDS_PORT=${{ secrets.RDS_PORT }}

            MONGODB_URI=${{ secrets.MONGODB_URI }}
            MONGODB_DBNAME=${{ secrets.MONGODB_DBNAME }}
            MONGODB_COLLECTION=${{ secrets.MONGODB_COLLECTION }}
            S3_BUCKET_NAME=${{ secrets.S3_BUCKET_NAME }}
            EOF

            echo "Starting Airflow services"
            docker-compose up -d --build
            docker-compose exec airflow airflow db init

            echo "Checking if Airflow admin user exists..."
            EXISTING_USER=$(docker-compose exec airflow-scheduler airflow users list | grep -c "${{ secrets.AIRFLOW_ADMIN_USERNAME }}")

            if [ "$EXISTING_USER" -eq "0" ]; then
              echo "Admin user does not exist. Creating a new admin user..."
              docker-compose exec airflow-scheduler airflow users create \
                --username "${{ secrets.AIRFLOW_ADMIN_USERNAME }}" \
                --firstname "${{ secrets.AIRFLOW_ADMIN_FIRSTNAME }}" \
                --lastname "${{ secrets.AIRFLOW_ADMIN_LASTNAME }}" \
                --role Admin \
                --email "${{ secrets.AIRFLOW_ADMIN_EMAIL }}" \
                --password "${{ secrets.AIRFLOW_ADMIN_PASSWORD }}"
            else
              echo "Admin user already exists. Skipping creation."
            fi

            docker-compose restart

          
